[["index.html", "AI Unleashed: Navigating the Risks of Our Automated Future 1 Introduction The Promises of AI The Risks of AI Our Aim", " AI Unleashed: Navigating the Risks of Our Automated Future Sierra Brooks, Austen Hardie, Griffin Hockenberry, Blake Waldman, Melissa Wolff 2023-08-18 1 Introduction The Promises of AI “Artificial Intelligence (AI) stands on the precipice of dramatically reshaping our world, unlocking potentials that have been largely the domain of science fiction. AI has the capability to revolutionize sectors from healthcare to finance, automating and enhancing tasks with a speed and accuracy that surpass human ability. It can predict patterns in complex data, opening doors for breakthroughs in medical diagnoses and climate modeling. By harnessing the power of machine learning, we can create more personalized user experiences, from smart homes to bespoke online content. AI can also boost economic productivity, catalyzing growth, and fostering innovation.” Actually, the entire paragraph you just read was written by ChatGPT. Cool, right? The Risks of AI Artificial intelligence, AI, is impressive in simple terms, but its capacity to mimic human thought and behavior doesn’t come without risks. Artificial intelligence can impact everyday life both in a positive and negative way. It has the possibility to impact the environment negatively, as well as increase the digital divide that is already present in modern day society. Along with this, humans may lose a sense of privacy and security over their own data and life. It’s also critical to consider what can happen when the AI space has a small number of powerful entities. These firms may be increasingly likely to leverage their own technologies to move and educate people in ways that they desire. Furthermore, as AI technology develops and becomes more accurate and accessible, society could become overly reliant on it to complete mundane tasks. Considering this, there may be incredible interruptions in our society when the technology eventually ‘breaks’ or has errors. Our Aim We plan to dive into some of the biggest risks and challenges that our world faces today because of AI’s development. "],["overview.html", "2 Overview", " 2 Overview It is easy to become enamored with the idea of artificial intelligence. After all, who wouldn’t be enticed by the idea of having a machine complete a task for you in 10 seconds that may have taken you an hour to complete. However, artificial intelligence isn’t without its risks. The purpose of this paper is to not necessarily prove that artificial intelligence is definitively bad, because it’s not, but rather bring to light some of the challenges that arise for humans as a result of the increased use of artificial intelligence. Firstly, we will examine the implications that AI has on the environment. Specifically, we will analyze how much water and energy AI consumes as well as examining the processes required to build sustainable AI. Next, we will take a look at the consequences of AI with regards to privacy and security. Where is the training data that AI utilizes coming from? How does generative AI impact data security? This section of the paper aims to answer these questions and provide insight into how AI can affect personal privacy and security. After discussing privacy and security we will address the digital divide that comes with the use of AI. in this section we focus on how different regions’ accessibility to AI differs and the consequences this might have on certain portions of the population. In this next section we switch to a focus on AI in the context of business. Specifically we will tackle how the capitalization of AI may negatively affect smaller companies and the repercussions this might have on market concentration. The last fundamental subject we will analyze is AI as infrastructure. What happens when AI breaks? If we have an overreliance on artificial intelligence what are the consequences when these machines fail? In this section we will examine the ensuing disruption that takes place when AI cannot function properly. In the last section of our paper we will address, but not dive deep into, other ways in which AI can pose potential threats. We compare and contrast the abilities humans possess versus those that machines do, then we briefly discuss the consequences of biased information in AI as well as AI’s potential in weapon development before concluding our paper with some legal and integration risks of AI. "],["green-ai-the-environmental-footprint.html", "3 Green AI: The Environmental Footprint Impact of AI on Environment Water Usage Energy Consumption Electronic Waste", " 3 Green AI: The Environmental Footprint Impact of AI on Environment In the field of sustainable development, AI is often touted as the end-all solution to optimization. AI does offer a lot of sustainability solutions, but it doesn’t come without its drawbacks. AI consumes a lot of resources to train its models. Water is consumed to cool the computers, energy is consumed to power the computers, and then the old hardware is thrown away. While AI offers a lot of promising advancements, being conscious of the negative effects is important to mitigating damage caused by AI. Water Usage One way in which AI can negatively affect the environment is through the massive amounts of water it consumes. Through energy generation for AI, training AI, and transmitting the data needed for AI, AI can consume a lot of water. It is estimated that training GPT3, the Large Language Model used by Chat-GPT, used 700,000 liters of clean fresh water (Guerrini 2023). Training AI also uses large amounts of electricity, and with our current generation mix, most electricity generation also consumes water. In most of the United States, this isn’t an issue, but most tech companies are located in the western United States, which is hit particularly hard by water scarcity and stress. As seen in the map below, published by the U.S. Global Change Research Program, water stress is almost entirely concentrated in the West. Water scarcity is a growing issue, it is estimated by the United Nations International Children’s Emergency Fund (Water 2023) that as early as 2025 half of the world’s population could experience some form of water scarcity. AI will have some positive impacts on water use. Allowing more precise water use in agriculture and efficient resource allocation in residential areas will help offset some of the negative impacts. As AI grows more prominent and more data is needed to train models its water use will grow exponentially but there are some methods we can use to reduce its negative impacts. By making conscious effects to minimize the water use of AI training and Data transfer, the impact can be significantly reduced. According to Forbes, Amazon, Meta, Google, and Microsoft, have pledged to become “water positive” by 2030 (Guerrini 2023). Energy Consumption As well as consuming water, AI consumes a lot of energy. Storing data, transferring data, and training models with that data are all energy-intensive tasks. As AI advances it will use more and more data and consume more and more energy. For example, in a report by Wired, they estimated that a recent Open AI model focused on solving Rubik’s cubes used 2.8 gigawatt-hours of energy to train which they equated to the same amount of power produced by three nuclear power plants in an hour (Knight 2020). While there was some doubt about the exact energy usage, all of the AI models like Chat-GPT or Dalle- 2 and most other widely used models likely consumed similar amounts of energy. In the United States, the energy generation mix is largely fossil fuels, with only 12% of energy being renewable (Guerrini 2023). This means increasing energy use is directly responsible for increased greenhouse gas emissions. While AI training can be made by some estimates up to 75% more efficient by some estimates, it is important to consider the energy use of AI and its effect on climate change (Champion 2023). Electronic Waste Another problem that comes with the increasing popularity of AI is electronic waste. E-waste is all the computing parts that are thrown away due to being broken, out of date, or obsolete. While the problem hasn’t hit AI extremely hard, a good model of what might happen is the e-waste associated with cryptocurrencies. It is currently estimated that Bitcoin “currently cycles through 30.7 metric kilotons of equipment per year” (Hern 2021). This amount of waste stacks up quickly. By bringing attention to this problem, it is possible to practice more sustainable AI development. "],["privacy-and-security-in-the-age-of-ai.html", "4 Privacy and Security in the Age of AI Introduction to AI Privacy and Security Concerns Data Collection and Management Security Protocols and Vulnerabilities Countermeasures and Defense Strategies Ethics and Legislation Societal Impact and Public Engagement Conclusion", " 4 Privacy and Security in the Age of AI Introduction to AI Privacy and Security Concerns The integration of AI into various aspects of daily life has brought about remarkable efficiency and convenience. However, the importance of privacy and security in AI cannot be overstated. Privacy refers to the right of individuals to control their personal information, while security focuses on protecting that information from unauthorized access (Zhang et al. 2017). The rise of AI has increased data collection and processing, fueling innovation while also sparking concerns about personal information misuse. Balancing AI capabilities with privacy and security is critical, requiring international collaboration and regulation to ensure responsible development that respects individual rights and societal values. Data Collection and Management Sources and Utilization of Data AI systems rely on large training datasets and can continuously collect data. These data sources vary and can include social media, online searches, mobile applications, and Internet of Things (IoT) devices. The rapid increase of connected devices, along with the growing number of people accessing the internet, has led to an explosion in the amount of data available for AI systems to analyze and learn from. Legal Frameworks and Compliance Companies often collect data through terms of service agreements, granting them extensive rights over user information. Legal and regulatory considerations that govern data collection are essential to ensure compliance with privacy and security standards. Frameworks such as the General Data Protection Regulation (GDPR) play a crucial role in guiding responsible data collection and usage (Parliament and European Union 2016). Transparency and User Consent A recent example of this practice is Zoom’s update to its Terms of Service, which initially led to concerns about the company’s use of personal data for AI training. The public reaction prompted Zoom to clarify that while it has rights for the users’ data, it will not currently use audio, video, or chat content for AI training (Walrath-Holdridge 2023). This incident highlights the importance of transparency and user awareness and underscores the need for clear communication with users about how their data is being used. Opportunities and Risks in Data Handling The diversity and complexity of data sources in AI present both opportunities and challenges. While access to rich data sets enables more sophisticated AI applications, it also increases the potential for privacy violations and security breaches Security Protocols and Vulnerabilities Encryption Techniques and Challenges Encryption plays a vital role in securing AI data by converting information into a code to prevent unauthorized access. One of the groundbreaking advancements in encryption is homomorphic encryption, which allows computations to be performed on encrypted data without first decrypting it (Gentry 2009). This ensures that sensitive information remains confidential even during processing, enhancing the privacy and security of AI applications. Homomorphic encryption has been applied in various fields, including healthcare and finance, where data privacy is paramount. However, the complexity and computational cost of homomorphic encryption remain challenges, necessitating ongoing research and optimization. Authentication Methods and Concerns Authentication in AI systems is essential to verify the identity of users and devices, ensuring that only authorized entities can access data and services. Biometric authentication, which uses unique physical characteristics such as fingerprints, is becoming increasingly popular in AI systems (Jain et al. 2004). Biometric authentication can be difficult to implement, but can enhance security compared to traditional password-based methods. However, it also raises privacy concerns, because biometric data is highly sensitive and must be handled with care. Integrating ways to ensure proper authentication, whether through biometric methods or other forms of verification, is vital as AI systems continue to grow. Identifying and Addressing Risks AI systems are not immune to vulnerabilities and potential risks. These can range from software flaws to hardware weaknesses, and even sophisticated new threats. Among these threats are: Quantum Attacks: Leveraging the capabilities of quantum computers to break widely used cryptographic algorithms, posing a significant threat to current encryption methods (Bernstein and Lange 2017). The development of quantum-resistant cryptographic techniques is essential to safeguard AI data. Adversarial Attacks: Malicious manipulations designed to deceive AI models. These can be broadly categorized into: Poisoning Attacks: Corrupting the training data, impacting the learning process itself and having long-term effects on AI models. Evasion Attacks: Altering the input to the AI model to produce incorrect outputs, typically targeted at specific instances to bypass security measures or manipulate AI-driven decisions (Biggio et al. 2013). Other Risks: Including unauthorized access, data breaches, and general weaknesses in software or hardware. The diverse nature of these threats requires a multifaceted approach to security. The transition to more robust security measures within AI systems is a complex and urgent task that requires collaboration between researchers, industry, and regulators to ensure the ongoing integrity and confidentiality of AI systems. Countermeasures and Defense Strategies Defending Against Attacks and Enhancing Robustness Defending against adversarial attacks requires a combination of countermeasures and defense strategies. Adversarial training, where the AI model is trained on adversarial examples, is one effective approach to enhance robustness (Madry et al. 2019). Other defense strategies include the development of detection mechanisms to identify adversarial inputs and the implementation of secure coding practices to minimize vulnerabilities. Collaboration between AI researchers, security experts, and policymakers is essential to develop comprehensive defense strategies that adapt to the evolving threat landscape. Ongoing Research and Future Directions The field of adversarial attacks and defenses is an active area of research, with ongoing efforts to develop robust AI models that can withstand sophisticated attacks (Carlini et al. 2019). Emerging research focuses on explainable AI, where models are designed to provide insights into their decision-making processes, enhancing transparency and accountability. Future directions also include the development of standardized testing and evaluation frameworks for AI security, fostering collaboration and knowledge sharing across academia, industry, and government. The dynamic nature of adversarial attacks necessitates continuous innovation and vigilance to ensure the integrity and trustworthiness of AI systems. Ethics and Legislation Existing Ethical Frameworks and Guidelines Ethical considerations in AI, particularly in the context of security and privacy, have led to the development of various frameworks and guidelines. One prominent example is the Asilomar AI Principles, which outline a set of ethical guidelines for AI research and development (Life Institute 2017). These principles emphasize the importance of safety, transparency, and accountability in AI systems, with specific attention to privacy and data protection. Different cultures, legal systems, and societal values may interpret ethical principles differently, leading to challenges in creating universally accepted guidelines. Challenges and Limitations in Ethical Compliance Enforcement and accountability are significant challenges in ethical AI. While some principles and guidelines already exist, ensuring compliance can be difficult. The rapidly evolving nature of AI technology, coupled with the complexity of privacy and security issues, makes enforcement a moving target. Additionally, the use of others’ data to create AI models creates questions related to plagiarism and copyright infringement. The boundaries between legitimate use and infringement are often blurred, leading to ethical dilemmas and potential legal conflicts. Future Directions in Ethical AI The future of ethical AI requires global collaboration and continuous research (Russell et al. 2016). Efforts must be made to harmonize ethical guidelines across different jurisdictions and to develop mechanisms for monitoring and enforcement. Ongoing research is also exploring the ethical implications of emerging technologies, such as federated learning, which aims to train AI models on decentralized data, potentially enhancing privacy and security. The intersection of ethics, law, and technology is a rich area for exploration, with the potential to shape the future of responsible AI development. Societal Impact and Public Engagement Social and Cultural Implications The ethical and societal dimensions of privacy and security in AI are complex and multifaceted. The potential for AI to worsen social inequalities through biased algorithms or to infringe on individual freedoms through excessive surveillance is real. Public debate and engagement are essential when ensuring that AI is developed and used in a manner that aligns with societal values and norms. Public Awareness and Educational Initiatives Public awareness and education play a pivotal role in the responsible development and deployment of AI. However, the technical complexity of AI can pose challenges in conveying these concepts without oversimplification. Promoting awareness in AI security and privacy requires a collaborative and multifaceted approach. Strategies may include public forums, workshops, online resources, and targeted educational programs. The goal is to create an informed public that understands the unique security and privacy challenges posed by AI, and is empowered to engage in meaningful dialogue and decision-making in these critical areas (National Academies of Sciences et al. 2017). Conclusion Summary of Key Findings The exploration of privacy and security in AI reveals a complex landscape, encompassing ethical frameworks, data protection measures, and societal implications. The integration of AI into daily life underscores the importance of privacy and security, illustrated by the role of encryption, authentication, and ethical considerations surrounding data collection and transparency. Call to Action The need for responsible AI development, legal compliance, ethical considerations, and public engagement is paramount. Collaborative efforts among policymakers, industry leaders, researchers, and the public are essential. Specific actions may include: Strengthening Regulations: Robust legal frameworks to protect privacy and ensure security. Promoting Transparency: Building trust through transparency in data handling. Enhancing Education and Awareness: Public education to empower informed decisions about AI. Fostering Collaboration: Interdisciplinary collaboration to address complex challenges. Future Prospects and Innovations Emerging technologies like quantum computing and federated learning present new opportunities and risks. Ongoing research is vital to harness the benefits of AI responsibly while recognizing inherent risks and ethical considerations. The exploration of privacy and security in AI is not merely technical challenges but societal endeavors. Responsible AI development requires understanding the intricate interplay between technology, ethics, law, and society. By fostering responsibility, transparency, and collaboration, we can ensure that AI aligns with societal values and legal norms, serving humanity with integrity and respect for privacy and security. "],["the-digital-divide-and-ai-accessibility.html", "5 The Digital Divide and AI Accessibility Introducing the Digital Divide AI and the Digital Divide Costs of AI - Economic Divide AI’s Changing Usefulness Looking Forward", " 5 The Digital Divide and AI Accessibility This page seeks to inform the reader about the societal impacts of AI, specifically looking at how populations may grow to be increasingly separate as technology impacts play out. Introducing the Digital Divide By definition, the digital divide is the gap between those with Internet access and those without it (Charlie Muller 2022). This term was coined in the mid-1990s, and as new technologies such as AI continue to emerge and develop, the digital divide only grows in new ways. Access to these technologies has come increasingly critical to a country or region’s economic, political, and social success. The digital divide seeks to explain that not everyone has access to the same technological resources, as this access fluctuates depending on various factors such as education, income, or race (Taylor 2023). Although various demographic or social factors can impact this divide, it’s also worth mentioning the two other substages of the digital divide that were first described by Jakon Nielson in 2006 (Nielsen 2006). The Types of Divides Nielsen believes the digital divide as it is normally used is synonymous with the economic divide as it is often focused on the affordability of the technology. The usability divide explains that many populations would not know what to do with a computer, even if they had free access to one. In the past, the groups most at risk here were the elderly and those with low literacy. The empowerment divide considers how we use technology to empower ourselves. This divide explains that humans need to be more aware of all of the technological capabilities computer systems actually have and feel comfortable using them. Given this information, the rapid emergence of Artificial Intelligence is likely to mimic the dispersion that occurred with the Internet and other emerging technologies. At this point, it is not possible to predict how AI will disperse and/or stick with populations, yet it is important to look at the current information we have about the world. As of April 2023, 64.6% of the global population makes up 5.18B internet users worldwide (Petrosyan 2023). In high-income countries, Internet usage is about 90% while it sits just above 25% for low-income countries. The graph below also shows the number of worldwide internet users over time. AI and the Digital Divide Although the digital divide has commonly been used to talk about the gap in internet access between populations, AI will likely have its own type of divide emerge rather soon. To be more specific, we are more likely to see populations that have a strong understanding of what AI is and how it can be used, use it more often. These groups will likely be able to afford it and will be able to access it as much as they want, especially because they’ll find the information valuable. On the flip side, many populations will remain uneducated about what AI is, which could be because of a variety of reasons. If an individual or group doesn’t have access to the internet, AI is irrelevant. At the same time, if they don’t have the infrastructure in place for AI to be useful, it’s unlikely to be adopted. Various barriers could limit AI’s impact in different spaces. In order to assess how much of a social disruption AI can be globally in terms of this potential socio-economic divide, it’s important to look at what countries are using AI the most right now. This current list is (higher percent usage listed first): United States, India, Canada, Germany, United Kingdom, Spain, Brazil, Mexico, Australia, Italy, France, and Japan (Watters 2023). Out of this list of twelve countries, only three of them are developing which are India, Brazil, and Mexico (Team 2023). This alone shows that AI is impacting the areas of the world that are already deemed as further developed, typically economically and/or technologically as those can go hand in hand. The United States, the country with the highest percentage of AI use in the world, still has a large portion of the population that is not fully aware of what AI actually is and what it can do beyond very simple capabilities. Even though this is the case, it’s interesting to look at how a strong understanding of AI doesn’t necessarily equate to trust in AI companies. Perception can vastly change depending on where an individual is from, which is shown in the chart below. Countries more open to AI may be quicker to adopt and those that aren’t may be slower. That slow adoption could be costly in the short or long term. These insights are helpful to think about, but at this point, it’s critical to further dive into AI’s current costs and accessibility. Costs of AI - Economic Divide For any business offering AI, the costs are incredibly high to both develop the product and maintain it. The cost ultimately depends on what the AI is intended to be used for, as larger business solutions tend to be more expensive than smaller use cases like ChatGPT’s chatbot by OpenAI. Earlier we described the economic divide, and the cost of AI falls right into this general bucket. Populations that are better off economically are in a better position to start utilizing AI tools, which in turn could make them even more productive or profitable than before. This will likely increase the gap between populations thriving financially and those simply getting by, as their global competitors will have resources they won’t be able to get their hands on. If we were to focus on what simple AI tools cost to an individual user, we’d look at what we’re seeing offered today. For example, Chat GPT is something every single university and school is talking about. Chat GPT is estimated to cost around $700,000 per day to OpenAI, in order to keep the tool running efficiently (Agomuoh 2023). Even though the costs are this high, the basic version of ChatGPT is free, while the paid version is $20/month. With resources like this that have lower user costs, accessibility between populations thankfully becomes less of a concern. However this is the case, the language with which the data has been trained becomes a larger issue. AI’s Changing Usefulness In the United States, AI’s impact has been large. Within a short span of time, individuals and businesses have been looking for ways to integrate AI into their lives strategically. This is because the vast amount of information and tasks that AI resources can contribute are relevant. By relevant, we mean that the information that the AI software was trained on is in the English language, a lot of it from an American perspective too. Given this, it’s important to consider how having AI trained in the English language could create global silos or further create divides in our society. Many may assume that English is spoken around the globe, but recent statistics show that 75% of the world’s population doesn’t speak English at all (Authors 2023). This fact means that 1 out of 4 people on the planet speak English. If most of the AI tools out there today are trained on data pertaining to English-speaking individuals, that means the majority of people will find these tools useless to at least some degree. Some AI tools may be language agnostic if it involves physically doing something, but AI used to mimic human thinking or ideation can be very off course if it wasn’t trained in the same culture or language. Direct translations aren’t always enough to make something connect with a different audience. Looking Forward It is easy to think of AI in silos, but if one is to anticipate how AI will change the world, these factors need to be considered. The large differences between cultures and regions will definitely impact AI’s spread and the openness to which it is adopted. But even as it is adopted, will AI hold the same value for a company in the US versus Europe versus Africa? Will areas with less exposure to new technology even understand how AI could change the way they go about life? Even if they understand the impacts, would they know how to use AI properly? If demand keeps increasing, who will be able to afford it and who will get pushed out? These are questions that need to be considered as AI begins to grow. "],["market-concentration.html", "6 Market Concentration Market Overview Power of Data Competition", " 6 Market Concentration Market Overview The AI market size is expected to increase more than $2 trillion over the next 10 years with tech giants leading the revolution. These companies hold tremendous power and have a responsibility to “foster innovation” (2023). The big tech corporations dominating the AI market include Alphabet, Microsoft, Apple, and Amazon. These firms have established their presence and invested vast resources into the future of AI. For example, Open AI lost $540 million in 2022 developing ChatGPT and DALL-E 2 (Chowdhury 2023). These AI algorithms require massive amounts of data and computing power, which can be costly. Without these two factors, models would essentially be useless. However, as the amounts of these factors increase, the smarter the model can become. Power of Data The data and power of the tech giants is unmatched, and the infinite data collection cycle that they operate under places them at an advantage (Niyazov 2023). They gather consumer data from the millions of users that they currently interact with, which can be used to improve AI algorithms and user experiences, ultimately attracting more users and retaining current ones. Competition Furthermore, as AI advances and human dependence on such tools increase, these firms will have greater power to control not only pricing but also the way that tasks are completed (Eliot 2020). These companies have essentially created an oligopoly within the industry, making it difficult for newcomers to enter the market. Certain AI startups have been successful in establishing their position within the market with the help of partnerships and acquisitions. For example, Intel acquired Habana Labs Ltd., a company focused on deep learning, and IBM Watson Health partnered with Guerbet, a medical imaging company (n.d.). Both of these unions were meant to promote the organizations’ presence within the AI market and expedite the rate at which these organizations could develop AI tools. Being the first to launch an AI product puts the company at a competitive advantage. Additionally, the competition among these top tech companies has pushed one another to be more innovative. After the introduction of ChatGPT, tech companies were fast to respond with their own versions of AI chatbots. Google created Bard, Apple partnered with Hugging Face to develop a similar platform, and various Chinese companies, including Alibaba and Tencent, have reportedly undertaken the same venture (Hart 2023). The acknowledgement of AI power has increased the demand for the development of such models, leading to a spike in demand for skilled professionals that surpasses the supply, posing another competitive factor in the AI race. In economics, a natural monopoly is where market conditions heavily favor the incumbent. The AI market exhibits many of these characteristics but with several companies, making it a natural oligopoly. A good example of a natural monopoly is public utilities (Eliot 2020). After there is infrastructure built at a large upfront cost, there is little incentive for new entrants to the market. This was the case with AT&amp;T until the 1980s. They had all the infrastructure to operate and it was too costly for new competitors to enter the market. This is what is happening with AI. As more and more data and computing power is needed to make competitive useful models, the cost to create increases quickly. This leads to the few companies that invested in AI early and kept up its development having control of all the AI models on the market. "],["when-ai-breaks-dependence-and-disruption.html", "7 When AI Breaks: Dependence and Disruption Introduction Definitions Theories and Consequences AI Failures", " 7 When AI Breaks: Dependence and Disruption Introduction AI is an impressive technological advancement that can aid in the progression of society. However, it is the responsibility of humans to manage and verify results generated by AI tools to combat overreliance and model drift. One purpose of AI is to help solve a problem. Today, AI can be found in virtually all industries, including healthcare, finance, and media; it can be used for simple tasks, such as setting reminders, or undertake more complicated responsibilities, like identifying fraudulent bank transactions. At the current state of AI, the best performing results arise when AI and humans work as a unit rather than individually. Humans and AI are both limited in their capabilities, thus collaborating to complete a task may ultimately result in a more ideal and efficient solution. Yet there are many factors that impact the way that humans interact with and depend on AI, leading to suboptimal performance. Definitions Overreliance: Overreliance is defined as accepting false information provided by AI as fact (Miller 2023). This can occur due to a variety of factors, including knowledge of the task to be performed, complexity of the work, and familiarity with the decision models behind the AI tool (Passi and Vorvoreanu, n.d.). Drift: Drift is a decrease in the number of accurate results produced by an AI model in one area due to the implementation of enhancements or fixes to other areas of the model. Dependence: Dependence from the perspective of the human-AI relationship, is when humans rely on AI to autonomously complete a task. Theories and Consequences Overreliance has a direct correlation to the complexity of the task to be performed and the ability of the user to perform such task without the help of AI. Humans often take AI generated results without questioning how such information was derived, which can be problematic; AI is not a foolproof solution. Theories suggest that overreliance can be combated by having AI generate an explanation of its finding along with the actual result. The logic behind these hypotheses is that the appearance of an explanation will caution users to validate the results as well as provide supporting evidence for how the output was generated. However, this conclusion is only partially true. Task complexity plays a significant role in whether or not the explanation aids in the users’ decision to affirm the output. A study performed by students and faculty at Stanford University and the University of Washington found a correlation between the complexity of the task and explanation of the AI – when a task is difficult, but the explanation is simple, overreliance decreases (Miller 2023). The study gave participants a maze and tasked them with finding the exit. Individuals could select from a range helpful AI generated tips, from detailed, turn by turn, written instructions to a simple image of the ideal path. Below are the example maze solutions. Which one would you choose? (Vasconcelos et al. 2023) Within both images above, the AI tool proposed a solution along with its reasoning, however, the written directions are more complicated to follow than the simple exit path drawing. The study concluded that the type of explanation is just as significant as the solution itself (Miller 2023). The explanation must be simpler than the task to be effective. The consequences of model drift can have a significant effect on operations. It can be argued that AI cannot be fully autonomous at its current state. Human intervention is needed to verify the accuracy of results and retrain models to fit the needs of the changing economic and social environments (3). Business requirements change and social norms evolve beyond what AI is capable of adjusting to without human input. One example of AI drift is ChatGPT’s decrease in accuracy in identifying prime numbers. New features were added to the model, thus resulting in this lag in performance (Vasconcelos et al. 2023). Due to the complexity of the AI models, development teams may have difficulty in identifying how to mitigate this drift and revert to prior performance standards. The introduction of AI has added significant value to society. It has been used to complete mundane tasks, ultimately giving humans more time to do other functions. As technology advances, societal reliance on these AI systems will increase. Dependence on AI leads to the idea of “use it or lose it” (2016). Humans may lose the ability to think for themselves or perform general chores that have been offloaded to AI. The continuous “replacement of human effort, physical, mental, and face-to-face social, with technology… is the erosion of human skill, engagement, attention, patience, persistence, and motivation, making it ever easier for artificial intelligence to replace human intelligence downstream, and not necessarily for our better” (2016). Humans must be aware of the impacts of AI in everyday life. AI Failures The use cases from AI are infinite. From improving healthcare delivery through early diagnosis and treatment to assisting in job performance, AI has the potential to benefit our way of life and culture. However, AI is not without faults; dependence on AI may lead to errors or may not even add any value. AI is only as powerful as its dataset and the development team. It is the responsibility of the user to verify the accuracy of the output until AI can be proven to produce only factual results. Below are examples of AI failures: During the height of the COVID-19 pandemic, AI models were being developed to assist medical professionals in identifying patients with the disease. The algorithms were developed based on inaccurate datasets produced by doctors who were unfamiliar with the disease. Furthermore, the development teams neglected key factors when training the model and cleaning the data, such as not removing duplicates and not inputting parameters to categorize patient sickness levels (Heaven 2021). Due to these factors, the COVID-19 AI models were deemed ineffective and useless in identifying the disease. Amazon invested in an AI tool to rank job applicants to assist the Human Resources department in hiring qualified candidates. The model was trained on data from previous applicants, which happened to be primarily male. Due to the male dominance among the dataset, the model learned to favor male applicants by rejecting resumes that had female undertones (Dastin 2018). Qualified female applicants were dismissed from the running and Amazon nixed further development of the tool. A Manhattan lawyer used ChatGPT to find supporting evidence for his case. He prompted the AI to provide a list of similar cases to the one he was working on. The platform was able to identify numerous cases that the lawyer added as support in his brief. The court was unable to verify any of these cases listed since all the cases from ChatGPT were fake and the lawyer neglected his due diligence, ultimately leading to the lawyer’s reputation being questioned and being tried (Weiser 2023). ChatGPT modifies and regurgitates information from the abundance of Internet content, resulting in text that it thinks should be put together. It is an impressive tool that has many use cases; however, its limitations must be realized. "],["the-rest-miscellaneous-ai-considerations.html", "8 The Rest: Miscellaneous AI Considerations Humans vs. Tech Bad Intentions", " 8 The Rest: Miscellaneous AI Considerations Humans vs. Tech Humans vs. Tech: Challenges AI Faces Against Humans With the increase in access to generative AI has come the stressful discussion about whether or not machines will be able to replace the human race. Enamored with the new found ability of machines, individuals are becoming increasingly nervous that they are replaceable. While there is some validity to these fears, there are still many skills and abilities humans possess that keep them at a higher intellectual level than artificial intelligence. These challenges that machines face are what separates artificial intelligence from becoming human. One of the most crucial differences between artificial intelligence and human intelligence is the ability to think (Simplilearn 2023). Artificial intelligence learns from data and processes such as machine learning and deep learning where information is constantly being fed into the machine in order to advance its knowledge. Humans on the other hand learn from thinking; reflecting on past experiences, analyzing abstract concepts and reasoning are all things that humans can do and machines cannot (Joshi 2022). Additionally, there are also skills that humans possess that machines never will be able to despite rapid technological advancements. For one, humans have personalities. While machine learning and deep learning are the ways in which a computer can learn to mimic a human’s actions, they cannot be trained to have a personality (Joshi 2022). Similar to this is the human ability to reason and pass judgment logically. Although machines can regurgitate or reframe existing information, they aren’t able to develop new theories or ideas on their own; this skill still remains exclusive to humans (Joshi 2022). Moreover, humans have the ability to pass judgements based on values and ethics. We can decipher what is right from what is wrong when we are calculating our decisions. Machines on the other hand don’t have the capacity to reason on a philosophical level separating them from their human counterparts (Marr 2022). Humans vs. Tech: Challenges Humans Face Against AI Despite the certain limitations AI faces, there is no doubt that this machinery still poses a threat to the human race. While AI may not be able to possess the advanced logical thinking skills humans have, there is no doubt that artificial intelligence can replace certain job positions that were once tailored exclusively to humans. A machine’s ability to compute and analyze large amounts of data in a short amount of time gives it a significant advantage over a human who may need a week to complete tasks that AI can do in 5 minutes (Thomas 2023). Bad Intentions Biased Information Additionally, there are certain dangers AI poses in a social aspect. AI algorithms used in applications such as TikTok generate new content for individuals based on what the machine thinks the human is most interested in (Thomas 2023). This poses a significant problem since AI’s assumptions about what humans want to see isn’t always accurate and can lead to the production of certain tailored content that may or may not be true (Thomas 2023). This also encapsulates another risk of AI which is its lack of transparency with regards to where it is gathering its data from (Thomas 2023). If these sources are biased then the information humans view through these artificial intelligence programs isn’t entirely accurate which can lead to misinformation. Weapon Development One of the more severe unintended consequences of artificial intelligence is the development of artificial intelligence powered weapons. While this may seem like a stretch, in reality it is actually far from it as illustrated through the actualization of Lethal Autonomous Weapon Systems (Thomas 2023). In fact, we can already see the beginning of these advancements through the Israeli Harpy Drone which essentially moves and locates a specific target before killing the individual (Lee 2022). We’ve seen something similar before in history when we switched from landmines in warfare to the development of nuclear weapons (Lee 2022). This fear has sparked debate over whether it is smarter to race into the AI arms battle or try to actively prevent it (Thomas 2023). These artificial intelligence powered weapons would prove to be smarter, stronger and more powerful than any alternative human power weapon, which despite being a more effective warfare weapon, could have much more severe implications (Lee 2022). Thus, while there is the possibility that these weapons could have beneficial results in terms of warfare, they also pose very serious and very deadly implications in the greater scheme of things. Legal Challenges An often overlooked challenge of artificial intelligence comes from the potential legal repercussions of using this software. Artificial intelligence has the ability to create new content off of existing data. Thus, when companies are looking to distribute this AI generated content it is crucial to make sure they have all the necessary patents, licenses or copyright information to do so, otherwise this could result in intellectual property theft (Marr 2023). Moreover, it might be necessary to adjust current laws or create new ones to address the implications of using AI (Marr 2023). This is also essential as it helps determine liability in situations where there are severe consequences to utilizing artificial intelligence (Marr 2023). Integration Risks One of the most alluring qualities of artificial intelligence is the ability to integrate into systems in order to standardize processes and in most cases make these processes more efficient. However, what is often left out is the consequences that ensue when integration fails or isn’t properly facilitated. Additionally, it is often easy for individuals to forget that artificial intelligence isn’t always correct, and thus they are less likely to override systems enabled by this software as they believe that the machine is always correct (Cheatham, Javanmardian, and Samandari 2019). This could lead to disastrous consequences such as, in the case of the self driving car example, a disastrous collision (Cheatham, Javanmardian, and Samandari 2019). This is also a result of humans’ lack of knowledge on artificial intelligence, not knowing the full capabilities of the machine as well as its weaknesses can result in sub-optimal human decisions. "],["team-dynamics-the-agile-way.html", "9 Team Dynamics: The Agile Way Introduction to Assignment and Topic Selection Project Planning and Timeline People and Interactions over Processes and Tools Working Software over Comprehensive Documentation Customer Collaboration over Contract Negotiation Responding to Change over Following a Plan", " 9 Team Dynamics: The Agile Way Introduction to Assignment and Topic Selection For this assignment, we were asked to create a collaborative document or guide to help the Ross community that is accessible via the web. More specifically, we were to create a website utilizing GitHub and RMarkdown within RStudio. We initially pondered a wide variety of topics that touched on generative AI, but after meeting with Sanjeev Kumar, we ended up focusing on the Risks and Challenges of AI. This meant that our audience would now shift. Instead of creating a guide for Ross students or faculty, we instead were creating a guide for anyone interested in AI. We made the assumption that most individuals do not know the ins and outs of AI, so we tried to remain high-level and anticipate the types of questions they would have. In addition, we tried to think about risks or narratives surrounding AI that they may already have exposure to, and try to further educate them in unique ways. After selecting a topic, we created a game plan for the rest of the semester. We wanted to ensure that we finished our deliverable on time and told a story using quality research, all while expanding our individual skill sets the entire time. Project Planning and Timeline After we had our topic, our first team meeting was spent coming up with possible subtopics and assigning those to the members of our group based on whose background most closely fit with the specified topic. Subsequent meetings were used as checkpoints where we refined goals and deadlines based on what we each thought we could accomplish in the coming week. We found it incredibly helpful to set agendas before meetings and specifically assigning anything that needed to be completed beforehand. A structured foundation from the beginning was essential in ensuring our team’s success. It provided us with the trust and shared goal that we needed moving forward to help create our website. People and Interactions over Processes and Tools One of the most fundamental ideals of the agile methodology is the importance of people and interactions over processes and tools. Throughout the duration of this project, our team undoubtedly relied more on one another than we did RStudio or GitHub. Despite having manuals that guided us through the basics of R Markdown and Git, our group utilized each other’s skills more often than we did the documentation. We learned from each other and failed together which naturally created a psychologically safe environment. Being able to work together in such a positive atmosphere allowed us to meet deadlines and work more efficiently. This can also be attributed to the fact that we all had a shared mission. We knew what we wanted to accomplish with our project and how to achieve it; thus, this consensus allowed us to have a very fluid group dynamic that proved to be extremely beneficial. We delegated tasks to each other and each individual within our group knew what was expected from them at each meeting. Additionally, since we were very open with one another regarding our strengths and weaknesses from the beginning of the project, this enabled us to be more transparent regarding which tasks we were able to complete and which ones we would need help with from the group. Ultimately, taking an agile approach to this project was advantageous for our group and we believe that this is exemplified through our final product. Working Software over Comprehensive Documentation We created this book using R Markdown, a package within RStudio that supports the building of html pages, and Git, a tool for version control and file sharing. These tools, especially Git, were useful when working in a team. Git provided an outlet for us to store our work and revert to previous versions if needed. Most of our team was new to R Markdown and Git, however, we used online resources as well as peers to help us learn. It was also helpful that one team member had previous experience using these tools. It was important for us to meet in person as we found this was the most productive approach to accomplishing our tasks. Referencing the agile manifesto, “working software over comprehensive documentation”, we focused our efforts on ensuring that after each push, the site was still functioning as expected. If team members received errors when interacting with Git or if a recent commit caused breakage to the site, we focused on fixing the issue. In case these errors occurred again, we jotted down notes on how to resolve it rather than spending time to fully document it with analyses on the various triggers that may cause it and the different mitigation approaches. This allowed us to work faster and concentrate on completing the task at hand. The short notes also served as a helpful reference when the errors would arise again. Customer Collaboration over Contract Negotiation For this project, we consulted with a wide variety of individuals. It was important that we thought of this project almost as a consulting assignment, where there was a client involved. This was not the actual case, but it was very helpful to think about our work from this point of view. Rather than assuming our audience had the same exact knowledge, experience, and thought process as our team did, we tried to think of our client or end audience as someone else. This was someone who did not know a lot about AI, or someone who only knew about how great AI was. In fact, the reason we were able to reach the conclusion of thinking of our audience as someone different than ourselves was due to User Stories. In class, we met with 2 other students from different project teams and were able to pitch our idea to them. They provided us with questions, general thoughts, and concerns surrounding our topic, which had a strong impact on what our end product looked like. In addition to these stories, our team meetings and meetings with Sanjeev were just as important. In meetings with Sanjeev, we learned to think outside of the box as our limited exposure to AI can significantly limit our ability to think about it creatively. We weigh our experiences with tools such as ChatGPT too heavily and don’t always think about the bigger picture. In addition, Sanjeev further emphasized the importance of telling a story from our research. Instead of researching blindly for surprising facts about AI risks or challenges, we could research with genuine curiosity about how our group’s narrative would shift because of the information we found. Lastly, internal meetings were crucial to our team’s success. This was a time when we worked on critical assignment milestones together, but also a time when we pushed each other to think about something differently. Instead of looking at a finished product and saying, “It looks great,” we were more likely to bring up some concerns or clarifying questions. Oftentimes, these small ‘just a thought’ comments ended up significantly changing our project’s trajectory for the better. As a whole, actively getting a diverse group of individuals involved in our project in different ways was highly beneficial. Since we told individuals that we wanted their raw and genuine thoughts, we got responses that were less formulaic. These thought-provoking insights only helped us more. Responding to Change over Following a Plan While working on this project, our team encountered a few unexpected challenges that required us to focus on adaptability. Our initial assumptions that the public generally held positive views on AI were challenged by our survey results. These results revealed people were well aware of some of the potential risks with AI, prompting a shift in the book’s structure. By employing Agile principles, we focussed on our audience and emphasized collaboration and flexibility, leading to successful adjustments in planning and execution. Tools like Google Docs and GitHub enhanced this flexibility and iterative development. Furthermore, our collaboration with Dr. Sanjeev Kumar and our keen attention to the audience’s expectations helped align our project with stakeholder needs. Despite having our occasional differences, the team’s dedication to open dialogue ensured cohesive progress. This project taught us the importance of being adaptable, emphasized the value of including diverse perspectives, and steered our focus towards prototype development. These insights, which closely align with key Agile Analytics principles, offer valuable guidance for future projects. Throughout the process, we embraced change; and while these shifts might initially seem troublesome, they often lead to marked improvements. "],["about-us.html", "About Us Meet the Team!", " About Us Meet the Team! Analytical Business Graphs Make Sense We’re a dedicated and driven team focused on leveraging the power of data analytics to deliver business insights. Working collaboratively, we aim to make sense of complex business data through insightful graphical representations. Sierra Brooks Hi! I am Sierra, a current Master of Business Analytics student from the suburbs of Chicago, specifically Plainfield, Illinois. I recently graduated from the University of Michigan with my Bachelor of Business Administration from the Ross School of Business. Throughout my undergrad, I completed various consulting and marketing internships as well as being heavily involved in the athletics community at Michigan. I am a current member of the Michigan Women’s Gymnastics team and will be going into my third year as the President of Michigan’s Student-Athlete Advisory Committee. When I have additional time, I enjoy photography, reading, watching movies, and exercising. I also adore podcasts and specifically love learning about social and decision sciences, education, and personal development. I have a younger brother and a 2 month old dog named Bruno. For more about me, visit my Austen Hardie I am a current Masters of Business Analytics student at the Ross School of Business from Westchester County, NY. I graduated from University of Michigan College of Literature, Science and Arts this past May (’23) with a Bachelors of Science in Mathematics. Last summer I interned at a software company in Boston and this was my first real introduction to the world of data analytics. I had a wonderful experience and it undoubtedly contributed to my reasons for applying to the MBAn program. I have experience coding in both C++ and Python as well as MATLAB and Simulink. I am the middle child of three girls, my older sister Hannah is 26 and my younger sister Emma is 18. I have a German Shepherd name Gauge who seems like the devil but is a sweetheart in reality. In my free time I enjoy going to workout classes, running, playing board and/or card games with my friends, cooking and baking. For more about me, visit my Griffin Hockenberry I am a current Master of Business Analytics student at the Ross School of Business. I grew up in the suburbs of Maryland outside of Washington D.C. In May of 2023, I completed a Bachelors of Science in Economics and Environmental Policy from the University of Michigan’s College of Literature, Science, and Arts. During my undergraduate degree, I was involved in Model United Nations at the University of Michigan (MUNUM) and M-etrics, a club where members would help teach each other data analytics/econometric techniques. In 2021, I completed a finance internship at a small government contractor working with satellites in Frederick, Maryland called Knight Sky. In my free time I like to listen to music, play guitar, and hike, especially if I can make it to a National Park. I am the middle sibling with an older and younger brother. After graduating from the MBAn program, I hope to work in environmental policy or Impact analysis. For more about me, visit my Blake Waldman I’m currently pursuing a Master of Business Analytics at the University of Michigan’s Ross School of Business. I recieved a BS from Case Western Reserve University with majors in Finance and Data Science and a minor in Economics. During my time at Case Western, I served as a teaching assistant for Statistics and managed a significant part of The Weatherhead Fund’s portfolio. Additionally, I held leadership roles in Alpha Kappa Psi and Zeta Beta Tau fraternities, enhancing my team management and strategic planning skills. Proficient in languages such as Java, Python, R, and SQL, I’m also experienced in tools like Google Suite, Microsoft Office, SPSS, NoSQL databases, and XML. My passion lies in leveraging the power of data to drive business decisions. Additionally, I am from Westchester County, NY, and I enjoy fishing, baking, and racket sports. For more about me, visit my Melissa Wolff I am a current Master of Business Analytics student at the Ross School of Business. I graduated from Binghamton University with a Bachelor of Science in Business Administration with a concentration in Management Information Systems and a minor in Computer Science. I have had multiple internships and jobs within the technology field in the entertainment and financial industries. Through my education and professional experience, I have become more interested in how to use data to benefit companies. In my past experience, I have had exposure to various programming languages, such as Python, C++, JavaScript, and SQL. I have designed and managed databases as well as automated manual processes to improve organizational efficiencies. I am from Binghamton, NY and the youngest of 3 (I have an older brother and older sister). I love roaming the streets of New York City and hope to move there after graduation. I enjoy baking and trying new recipes, doing the New York Times crossword puzzle, exercising, and playing the piano. For more about me, visit my "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
